{
  "model": "meta-llama/Llama-2-70b-hf",
  "device": "cpu",
  "dtype": "float16",
  "max_model_len": 4096,
  "tensor_parallel_size": 1,
  "pipeline_parallel_size": 1,
  "gpu_memory_utilization": 0.9,
  "swap_space": 4,
  "cpu_offload_gb": 0,
  "enable_prefix_caching": true,
  "enable_lora": false,
  "max_lora_rank": 16,
  "max_num_seqs": 256,
  "max_num_batched_tokens": 2560,
  "tokenizer_mode": "auto",
  "tokenizer_pool_size": 0,
  "skip_tokenizer_init": false,
  "block_size": 16,
  "use_v2_block_manager": false,
  "trust_remote_code": true,
  "download_dir": "/root/.cache/huggingface/hub",
  "load_format": "auto",
  "disable_log_stats": false,
  "disable_log_requests": true,
  "served_model_name": "llama-2-70b",
  "chat_template": null,
  "api_key": "sk-llm-bot-local",
  "ssl_keyfile": null,
  "ssl_certfile": null,
  "ssl_ca_certs": null,
  "ssl_cert_reqs": 0,
  "quantization": null,
  "enforce_eager": false,
  "max_context_len_to_capture": 8096,
  "seed": 0,
  "worker_use_ray": false,
  "distributed_executor_backend": "ray"
}

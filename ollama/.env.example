# Ollama Configuration
# Copy this file to .env and customize for your environment

# Memory and CPU Configuration
# ============================

# Maximum memory for Ollama container (in bytes or use suffixes: k, m, g)
# Examples: 4g, 8GB, 2048m
OLLAMA_MEMORY_LIMIT=4g

# Number of CPU cores to limit (optional)
# If not set, uses all available cores
# Examples: 2, 4, 8
OLLAMA_CPUS_LIMIT=4

# Number of parallel model loading operations
# Default: 4
OLLAMA_NUM_PARALLEL=4

# Number of threads to use per request
# Leave empty to auto-detect based on available CPU
OLLAMA_NUM_THREADS=4

# Context length for model inference (in tokens)
# Higher values use more memory
OLLAMA_NUM_CTX=2048

# Model to preload on startup (optional)
# Examples: llama2, mistral, neural-chat
OLLAMA_MODEL=llama2

# Port Configuration
# ==================

# Port to expose Ollama API
OLLAMA_PORT=11434

# Docker Configuration
# ====================

# Image tag to use
OLLAMA_IMAGE_TAG=latest

# Container name
OLLAMA_CONTAINER_NAME=ollama

# Volume mount path for model cache (optional)
# Leave empty to use anonymous volume
# Examples: /var/ollama, ./models
OLLAMA_VOLUME_PATH=

# Logging
# =======

# Log level (debug, info, warn, error)
OLLAMA_LOG_LEVEL=info

# Advanced Options
# ================

# Enable mmap mode for faster model loading
OLLAMA_MMAP=1

# Keep models in memory (1) or unload when idle (0)
OLLAMA_KEEP_ALIVE=5m

# GPU support (disable for CPU-only)
# Set to 0 to force CPU-only mode
OLLAMA_GPU_LAYERS=0
